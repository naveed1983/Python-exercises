{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Week 2 - Polynomial Curve Fitting and Decision Theory"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Learning contents:\n",
    "\n",
    "1. Linear models\n",
    "    - Linear function\n",
    "    - Error function\n",
    "    - Root meant square error\n",
    "    - Optimization of Error function\n",
    "    - Test the model\n",
    "2. Regularization\n",
    "    - Error function\n",
    "    - Optimization\n",
    "    - Test with regularization\n",
    "3. Model Selection\n",
    "    - Cross-validation\n",
    "4. Bayesian curve fitting\n",
    "    - Display results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Description"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this exercise we will apply Linear Models for Polynomial Curve Fitting task.\n",
    "\n",
    "You have to fill empty functions (with pass in body) to match their purpose.\n",
    "\n",
    "1. You have to create a code for evaluation of a Linear Model, it's error functions and finding optimal weights with given error functions.\n",
    "2. You need to add regularization to the optimization procedure.\n",
    "3. You need to implement Cross-validation model selection technique.\n",
    "4. You need to implement Bayesian curve fitting, computing phi and S matricies at first, then using them to compute mean and variance."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "from math import  exp\n",
    "\n",
    "import seaborn as sns; sns.set(); sns.set_palette('bright')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def target_func(x): return np.sin(2*np.pi*x)\n",
    "\n",
    "def generate_data(size):\n",
    "    rng = np.random.RandomState(26052605)\n",
    "    x_train = rng.uniform(0., 1., size)\n",
    "    y_train = target_func(x_train) + rng.normal(scale=0.1, size=size)\n",
    "    x_test = np.linspace(0., 1., 100)\n",
    "    y_test = target_func(x_test)\n",
    "    \n",
    "    return x_train, y_train, x_test, y_test\n",
    "\n",
    "x_train, y_train, x_test, y_test = generate_data(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(x_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(x_test, y_test, '-')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1) Linear models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1) Linear function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Complete the function below, named `linear`, which takes two parameters: a single data point `x` and a list of `M` number of `weights`. The function should return an output value `y` as given in the equation on slide 6 of Lecture 3. In essence, this function should implement a polynomial of order `M`.   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def linear(x, weights):\n",
    "    pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2) Error function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Complete the function below, named `err`, that computes the sum-of-squared error between the output of the function above and the corresponding target value. Specifically, the function should implement the error equation in slide 7 of Lecture 3. The function takes `weights`, `inputs` and `targets` as parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def err(weights, inputs, targets):\n",
    "    pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.3) Root meant square error"
   ]
  },
 {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Write a function for computing the root mean squared error as given in the equation in slide 12 of Lecture 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def erms(weights, inputs, targets):\n",
    "    pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.4) Optimization of Error function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create a function below that obtains `optimial_weights` by implementing the optimization solution given in slide 9 of Lecture 3. The function takes the following parameters: `inputs`, `targets`, and `M` (number of weights) as parameters and returns optimal weights for the given set of training data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def optimial_weights(inputs, targets, M):\n",
    "    pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.5) Test the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_all(start_M, end_M, x_train, y_train, x_test, y_test):\n",
    "    \n",
    "    results_train = []\n",
    "    results_test = []\n",
    "    all_weights = []\n",
    "    \n",
    "    for M in range(start_M, end_M + 1):\n",
    "        weights = optimial_weights(x_train, y_train, M)\n",
    "        all_weights.append(weights)\n",
    "        error_train = erms(weights, x_train, y_train)\n",
    "        error_test = erms(weights, x_test, y_test)\n",
    "        results_train.append(error_train)\n",
    "        results_test.append(error_test)\n",
    "    return results_train, results_test, all_weights\n",
    "\n",
    "r_tr, r_tt, all_weights = test_all(0, 9, x_train, y_train, x_test, y_test)\n",
    "\n",
    "plt.plot(list(range(0, 10)), r_tr, '-o', label='train')\n",
    "plt.plot(list(range(0, 10)), r_tt, '-o', label='test')\n",
    "plt.xlabel('M')\n",
    "plt.ylabel('ERMS')\n",
    "plt.legend()\n",
    "plt.yscale('log')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Weights table for different `M`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(pd.DataFrame(all_weights))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Estimated curve for `M=9` (same as the amount of data points) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(x_test, list(map(lambda x: linear(x, optimial_weights(x_train, y_train, 9)), x_test)), '-')\n",
    "plt.plot(x_train, y_train, 'og')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2) Regularization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1) Error function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`err_regularization` takes `weights`, `inputs`, `targets` and `l` (regularization term) and computes sum-of-squares error with weights regularization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def err_regularization(weights, inputs, targets, l):\n",
    "    pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`erms_regularization` is a regularization version of a root mean squares error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def erms_regularization(weights, inputs, targets, l):\n",
    "    pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2) Optimization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`optimial_weights_regularization` takes `inputs`, `targets`, `M` (number of weights) and `l` (regularization term)  as parameters and returns optimal weights (with regularization) for this data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def optimial_weights_regularization(inputs, targets, M, l):\n",
    "    pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3) Test with regularization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_all_regularization(ls, M, x_train, y_train, x_test, y_test):\n",
    "    \n",
    "    results_train = []\n",
    "    results_test = []\n",
    "    all_weights = []\n",
    "    \n",
    "    for l in ls:\n",
    "        weights = optimial_weights_regularization(x_train, y_train, M, l)\n",
    "        all_weights.append(weights)\n",
    "        error_train = erms_regularization(weights, x_train, y_train, l)\n",
    "        error_test = erms_regularization(weights, x_test, y_test, l)\n",
    "        results_train.append(error_train)\n",
    "        results_test.append(error_test)\n",
    "    return results_train, results_test, all_weights\n",
    "\n",
    "ls = [0, exp(-18), exp(-5), exp(0)]\n",
    "\n",
    "r_tr_r, r_tt_r, all_weights_r = test_all_regularization(ls, 9, x_train, y_train, x_test, y_test)\n",
    "\n",
    "plt.plot(ls, r_tr_r, '-o', label='train')\n",
    "plt.plot(ls, r_tt_r, '-o', label='test')\n",
    "plt.xlabel('ln Lambda')\n",
    "plt.ylabel('ERMS_REGULARIZATION')\n",
    "plt.legend()\n",
    "plt.yscale('log')\n",
    "plt.xscale('log')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Weights for `M=9` with regularization terms `0`, `exp(-18)`, `exp(-5)`, `exp(0)`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(pd.DataFrame(np.transpose(all_weights_r)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_by_lambda(l):\n",
    "    plt.plot(x_test, y_test, '-m')\n",
    "    plt.plot(x_test, list(map(lambda x: linear(x, optimial_weights_regularization(x_train, y_train, 9, l)), x_test)), '-')\n",
    "    plt.plot(x_train, y_train, 'og')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_by_lambda(exp(0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_by_lambda(exp(-5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_by_lambda(exp(-18))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_by_lambda(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3) Model Selection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1) Cross-validation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`create_cross_validation_sets` takes `S` (number of sets) and data points `x_train`, `y_train` as parameters and returns array of sets in form `[x_sub_train, y_sub_train, x_validation, y_validation]` each"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_cross_validation_sets(S, x_train, y_train):\n",
    "    pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`best_model` takes `start_M`, `end_M` (low and high limits to search for `M`), `ls` (list of regularization terms) and `sets` (cross-validation sets) and should return `(top_M, top_l, top_result_test)` with `M`, `l` and result for the best model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def best_model(start_M, end_M, ls, sets):\n",
    "    pass\n",
    "\n",
    "\n",
    "x_cross_train, y_cross_train, _, _ = generate_data(100) \n",
    "\n",
    "M, l, r = best_model(0, 9, [0, exp(-18), exp(-5), exp(0)], create_cross_validation_sets(10, x_cross_train, y_cross_train))\n",
    "print('M =', M, 'lambda =', l, 'erms =', r)\n",
    "\n",
    "plt.plot(x_test, y_test, '-m')\n",
    "plt.plot(x_test, list(map(lambda x: linear(x, optimial_weights_regularization(x_train, y_train, M, l)), x_test)), '-')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4) Bayesian curve fitting (Optional)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`phi` takes `x` (data point) and `M` (number of weights) as arguments and returns a vector of powers of `x` from `0` to `M`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def phi(x, M):\n",
    "    pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`S` takes `alpha`, `beta`, `x` (all data points), and `M` as arguments and returns a matrix `S` that is used to compute `mean` and `variance`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def S(alpha, beta, x, M):\n",
    "    pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`mean` takes `alpha`, `beta`, `x_star` (new point), `x` (all data points), `t` (target values), and `M` and computes mean for the Gaussian distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mean(alpha, beta, x_star, x, t, M):\n",
    "    pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`variance` takes `alpha`, `beta`, `x_star` (new point), `x` (all data points), `t` (target values), and `M` and computes variance for the Gaussian variance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def variance(alpha, beta, x_star, x, t, M):\n",
    "    pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.1) Display results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "alpha = 0.05\n",
    "beta = 1.1\n",
    "M = 9\n",
    "\n",
    "means = np.array(list(map(lambda x: mean(alpha, beta, x, x_train, y_train, M), x_test)))\n",
    "variances = np.array(list(map(lambda x: variance(alpha, beta, x, x_train, y_train, M), x_test)))\n",
    "\n",
    "plt.plot(x_train, y_train, 'og')\n",
    "plt.plot(x_test, y_test, '-m')\n",
    "plt.plot(x_test, means, '-b')\n",
    "plt.fill_between(x_test, means + variances, means - variances, color='red', alpha='0.3')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
