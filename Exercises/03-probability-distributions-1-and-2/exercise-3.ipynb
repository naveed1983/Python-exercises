{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Week 3 - Probability Distributions 1 and 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Learning contents:\n",
    "\n",
    "1. Frequenist's approach\n",
    "    - Bernoulli distribution function\n",
    "    - Maximum Likelihood\n",
    "    - Display distribution\n",
    "2. Bayesian approach\n",
    "    - Binomial distribution\n",
    "    - Display Binomial distribution for different m\n",
    "    - Maximum posterior\n",
    "3. Multinomial variables\n",
    "    - Generate data\n",
    "    - Frequenist's approach\n",
    "        - Multinomial Bernoulli distribution function\n",
    "        - Maximum likelihood\n",
    "4. Gaussian distribution\n",
    "    - Distribution function\n",
    "    - Generate data\n",
    "    - Display data\n",
    "    - Frequenist's approach\n",
    "        - Maximum likelihood\n",
    "        - Display distribution\n",
    "    - Bayesian approach\n",
    "        - Generate data\n",
    "        - Variance is known, mean is unknown\n",
    "        - Display mean estimation for different N"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import random\n",
    "from math import exp, sqrt\n",
    "from collections import Counter\n",
    "from scipy.special import comb\n",
    "from scipy.stats import beta\n",
    "from scipy.stats import multivariate_normal, norm\n",
    "\n",
    "import seaborn as sns; sns.set(); sns.set_palette('bright')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The code listed below defines 2 functions: 'weighted_coin' gives an output of a random experiment involving a single toss of a loaded or weighted coin. The probability of getting 'heads' is 0.7, whereas the probability of getting 'tails' is 0.3. In the case of heads, the function outputs 1, else it outputs 0."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "random.seed(2605)\n",
    "\n",
    "def weighted_coin(heads=0.7, tails=0.3):\n",
    "    total = heads + tails\n",
    "    value = random.random() * total\n",
    "    result = 0 if value <= tails else 1\n",
    "    return result\n",
    "\n",
    "\n",
    "def generate_data(size):\n",
    "    return list(map(\n",
    "        lambda i: weighted_coin(),\n",
    "        range(size)\n",
    "    ))\n",
    "\n",
    "\n",
    "train_size = 10000\n",
    "train_data = generate_data(train_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "counter = Counter(train_data)\n",
    "plt.bar(counter.keys(), list(map(lambda a: a / train_size, counter.values())))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1) Frequenist's approach"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1) Bernoulli distribution function"
   ]
  },
 {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "write the function 'bern' that outputs the probability density function value of a bernoulli ramdom variable 'x' and paramater 'mu' (see slide 7 in lecture 5)."
   ]
  },

  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def bern(x, mu):\n",
    "    pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2) Maximum Likelihood"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "write the function mu_maximum_likelihood_bern that takes data points train_data and should return the Maximum Likelihood estimate of the parameter 'mu' of the Bernoulli distribution (slide 9 of lecture 5). Is your estimate of 'mu' close to the theoretical value? Also, convince yourself that the 'train_data' that you generated above does belong to Bernoulli distribution!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mu_maximum_likelihood_bern(train_data):\n",
    "    pass\n",
    "\n",
    "mu_ml = mu_maximum_likelihood_bern(train_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.3) Display distribution"
   ]
  },
{
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The function below plots the probability density (or mass) function of a Bernoulli random variable. Why is this function discrete?"   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def display_bernoulli(mu, color='b'):\n",
    "\n",
    "    #xs = np.linspace(0., 1., 100)\n",
    "    ys = list(map(\n",
    "        lambda x: bern(x, mu),\n",
    "        [0,1]\n",
    "    ))\n",
    "\n",
    "    plt.bar([0,1],ys)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display_bernoulli(mu_ml)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2) Bayesian approach"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1) Binomial distribution"
   ]
  },
{
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "write a function that gives the value of probability density function for Binomial random variable with input parameters 'm', 'N' and 'mu'"   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def binomial(m, N, mu):\n",
    "    pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2) Display Binomial distribution for different m"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def display_binomial(N, mu):\n",
    "    xs = list(range(N))\n",
    "    ys = list(map(\n",
    "        lambda m: binomial(m, N, mu),\n",
    "        xs\n",
    "    ))\n",
    "    \n",
    "    plt.bar(xs, ys)\n",
    "    plt.xlabel('m')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display_binomial(10, 0.25)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3) Maximum posterior"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "write a function 'mu_maximum_posterior_bern' which takes parameters 'm', 'N', 'a', 'b' and should return the Maximum Posterior estimate of the parameter 'mu' of the Bernoulli distribution (see the slide 18 in lecture 5). Compare this estimate with that obtained earlier in section 1.2 using maximum likelihood estimator. How the two estimators compare for large values of 'N'?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mu_maximum_posterior_bern(m, N, a, b):\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    " print(mu_maximum_posterior_bern(sum(train_data), train_size, 0.1, 0.1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3) Multinomial variables"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1) Generate data"
   ]
  },
{
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "generate samples of the multinomial random variable using 1-of-K scheme (see slide 21 of week 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(26)\n",
    "\n",
    "def generate_multinomial_data(size, probabilities):\n",
    "    numbers = np.random.choice(len(probabilities), size, 1, probabilities)\n",
    "    \n",
    "    result = np.zeros((numbers.size, len(probabilities)))\n",
    "    result[np.arange(numbers.size), numbers] = 1\n",
    "    return result\n",
    "    \n",
    "probabilities=[0.5, 0.1, 0.2, 0.1, 0.1]\n",
    "multinomial_data = generate_multinomial_data(100, probabilities)\n",
    "multinomial_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2) Frequenist's approach"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.2.1) Multinomial distribution function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "write a function that gives the value of probability density function of multinomial random variable, given the data xs and parameters mus (see slide 21 of week 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def bern_multi(xs, mus):\n",
    "    pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.2.2) Maximum likelihood"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`mu_maximum_likelihood_multinomial_bern' takes multinomial data points multinomial_data and should return Maximum Likelihood estimate of the parameters 'mus' of the Multinomial distribution. Print the estimated parameter values and compare those with the actual values. If they are different, how could you bring them closer?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mu_maximum_likelihood_multinomial_bern(multinomial_data):\n",
    "    pass\n",
    "\n",
    "multi_mu_ml = mu_maximum_likelihood_multinomial_bern(multinomial_data)\n",
    "multi_mu_ml"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4) Gaussian distribution"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.1) Multivariate Gaussian Distribution Function"
   ]
  },
{
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "write a function that gives the value of probability density function of Gaussian random variable, given the data point 'x' and parameters 'mean' and 'variance' (see slide 6 of week 6). Hint: use the 'pdf' method of 'scipy.stats.multivariate_normal' function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def multi_gaussian(x, mean, covariance):\n",
    "    pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2) Generate data"
   ]
  },
{
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "write a function to generate 'size' number of random samples from multivariate Gaussian distribution with specified mean and covariance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_multi_gaussian_data(size, mean, covariance):\n",
    "    return np.array(multivariate_normal(mean=mean, cov=covariance).rvs(size=size, random_state=26))\n",
    "\n",
    "target_mean = [1.7, -4]\n",
    "target_cov = [[1, 0.1], [0.1, 1]]\n",
    "multi_gaussian_data = generate_multi_gaussian_data(50, target_mean, target_cov)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.3) Display data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(multi_gaussian_data[:, 0], multi_gaussian_data[:, 1], '.')\n",
    "\n",
    "def display_gaussian_contour(data, mean, covariance, cmap='summer'):\n",
    "\n",
    "    lx = min(data[:, 0])\n",
    "    rx = max(data[:, 0])\n",
    "    by = min(data[:, 1])\n",
    "    uy = max(data[:, 1])\n",
    "\n",
    "    x, y = np.mgrid[lx:rx:.01, by:uy:.01]\n",
    "    pos = np.dstack((x, y))\n",
    "    plt.contour(x, y, multivariate_normal(mean, covariance).pdf(pos), cmap=cmap)\n",
    "\n",
    "display_gaussian_contour(multi_gaussian_data, target_mean, target_cov)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.4) Parameter Estimation using Frequenist's approach"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4.4.1) Maximum likelihood"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "write the `multi_gaussian_mu_maximum_likelihood` function that takes Gaussian distributed data points `multi_gaussian_data` and should return Maximum Likelihood estimate of the mean of Multivariate Gaussian distribution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def multi_gaussian_mu_maximum_likelihood(multi_gaussian_data):\n",
    "    pass\n",
    "\n",
    "g_mu_ml = multi_gaussian_mu_maximum_likelihood(multi_gaussian_data)\n",
    "g_mu_ml"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Write the function `multi_gaussian_covariance_maximum_likelihood` that takes Gaussian data points `multi_gaussian_data` and Maximum Likelihood estimate for `mu` (`g_mu_ml`) and should return Maximum Likelihood estimate of the covariance matrix of that distribution (slide 12 of lecture 6)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def multi_gaussian_covariance_maximum_likelihood(multi_gaussian_data, g_mu_ml):\n",
    "    pass\n",
    "\n",
    "g_cov_ml = multi_gaussian_covariance_maximum_likelihood(multi_gaussian_data, g_mu_ml)\n",
    "g_cov_ml"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4.4.2) Display distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display_gaussian_contour(multi_gaussian_data, g_mu_ml, g_cov_ml, cmap='spring')\n",
    "display_gaussian_contour(multi_gaussian_data, target_mean, target_cov, cmap='summer')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.5) Parameter Estimation using Bayesian approach"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4.5.1) Generate data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_gaussian_data(size, mean, variance):\n",
    "    return np.array(norm(mean, sqrt(variance)).rvs(size=size, random_state=26))\n",
    "\n",
    "g_mean = 0.8\n",
    "g_variance = 0.1\n",
    "gaussian_data = generate_gaussian_data(50, g_mean, g_variance)\n",
    "\n",
    "plt.scatter(gaussian_data, [0] * len(gaussian_data), alpha=0.5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4.5.2) Estimating mean distribution when the variance is known"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "write the function `multi_gaussian_mu_maximum_posterior` that takes data points `gaussian_data`, prior `mu0` and `variance0`, `variance` and should return Maximum Posterior estimate of the mean of Multivariate Gaussian distribution and its variance (see bottom 2 equations in slide 15 of lecture 6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def multi_gaussian_mu_maximum_posterior(gaussian_data, mu0, variance0, variance):\n",
    "    pass\n",
    "\n",
    "g_mu_map, g_mu_var_map = multi_gaussian_mu_maximum_posterior(gaussian_data, 0, g_variance, g_variance)\n",
    "g_mu_map, g_mu_var_map"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4.5.3) Display mean estimation for different values of N"
   ]
  },
{
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Looking at the plots below, why does the mean distribution gets more and more concentrated around true value as the number of samples increase? What benefits this Bayesian approach offers over maximum likelihood approach?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def display_mu_map(gaussian_data, mu0, variance0, variance):\n",
    "    g_mu_map, g_mu_var_map = multi_gaussian_mu_maximum_posterior(gaussian_data, 0, g_variance, g_variance)\n",
    "    x = np.linspace(-1, 1, 100)\n",
    "    y = norm(g_mu_map, sqrt(g_mu_var_map)).pdf(x)\n",
    "    plt.plot(x, y, label='N=' + str(len(gaussian_data)))\n",
    "    plt.legend()\n",
    "\n",
    "display_mu_map(gaussian_data[:0], 0, g_variance, g_variance)\n",
    "display_mu_map(gaussian_data[:1], 0, g_variance, g_variance)\n",
    "display_mu_map(gaussian_data[:10], 0, g_variance, g_variance)\n",
    "display_mu_map(gaussian_data, 0, g_variance, g_variance)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
