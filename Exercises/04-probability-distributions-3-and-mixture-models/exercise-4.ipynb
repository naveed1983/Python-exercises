{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Week 4 - Probability Distributions 3 and Mixture Models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Learning contents:\n",
    "\n",
    "1. Histogram-based density estimation\n",
    "    - Display histogram densities\n",
    "2. Kernel density estimation\n",
    "    - Hypercube Kernel function\n",
    "    - Gaussian Kernel function\n",
    "3. K-Nearest Neigbours classification\n",
    "    - Generate data\n",
    "    - Classification function\n",
    "    - Display results\n",
    "4. K-Means clustering\n",
    "    - Display results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.animation import FuncAnimation\n",
    "from math import sqrt\n",
    "from collections import Counter\n",
    "from scipy.stats import norm\n",
    "from sklearn import datasets\n",
    "from IPython.display import HTML\n",
    "from math import floor\n",
    "\n",
    "import seaborn as sns; sns.set(); sns.set_palette('bright')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate data"
   ]
  },
{
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "write a function `generate_data_1D` that returns `n` number of 1D random variables exhibiting mixture of Gaussian distribution with parameters `means`, `variances` and `pi`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_data_1D(size, means, variances, pi):\n",
    "    x=[]\n",
    "    np.random.seed(10)\n",
    "    for i in range(size):\n",
    "        z_i = np.argmax(np.random.multinomial(1, pi))\n",
    "        x_i = np.random.normal(means[z_i], variances[z_i]) \n",
    "        x.append(x_i) \n",
    "    return x"
   ]
  },
{
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The function below generates 1000 data points from the Gaussian mixture distribution and plots their histogram. Observe the plot and comment on whether the plot coincides with the input data."
   ]
  },
{
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "means = [1, 5, 9]\n",
    "variances = [1, 1, 1]\n",
    "pis = [0.4, 0.2, 0.4]\n",
    "data = generate_data_1D(1000, means, variances, pis);\n",
    "fig, ax = plt.subplots(figsize=(8, 4));\n",
    "ax.hist(data, bins=100, density=True);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1) Histogram-based density estimation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "write a function `histogram` that takes input `data`, user-defined bin size `delta` and returns `bins` (list of edges of the bins) and their corresponding `probabilities`. Hint: you can use numpy's histogram function to do so."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def histogram(data, delta):\n",
    "    pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1) Display histogram densities"
   ]
  },
{
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Display the `histograms` for different values of bin sizes and also compute and plot the `true` probability density (slide 9 of lecture 7) of the data. Compare the two plots and see which values of bin sizes reveal densities that are close to the true density function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
   "def univariate_normal(x, mean, variance):\n",
   "    return ((1. / np.sqrt(2 * np.pi * variance)) * np.exp(-(x - mean)**2 / (2 * variance)))\n",
   "\n",
    "def display_histogram_density(data, delta):\n",
    "    bins, probs = histogram(data, delta)\n",
    "\n",
    "    a = np.arange(-5, 15, 0.01)\n",
    "    y = pis[0] * univariate_normal(a, mean=means[0], variance=variances[0]**2) + pis[1] * univariate_normal(a, mean=means[1], variance=variances[1]**2)+ pis[2] * univariate_normal(a, mean=means[2], variance=variances[2]**2)\n",
    "    fig, ax = plt.subplots(figsize=(8, 4))\n",
    "    ax.bar(bins[:-1],probs,width=0.1)\n",
    "    ax.plot(a, y,'-r')"
]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display_histogram_density(data, 0.05)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display_histogram_density(data, 0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display_histogram_density(data, 0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display_histogram_density(data, 1.5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2) Kernel density estimation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1) Hypercube Kernel function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "write a function `hypercube_kernel_function` takes `u` and returns 0 or 1 if `u` is inside 1/2 hypercube (see slide 18 of lecture 7)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def hypercube_kernel_function(u):\n",
    "    pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "write the function `hypercube_kernel_density` that takes a single data point `x`, training data points `data`, size of a cube `h`, data dimensions `D` and returns the probability density of `x` based on the Hypercube kernel function (see the first equation on slide 19 of lecture 7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def hypercube_kernel_density(x, data, h, D):\n",
    "    pass"
   ]
  },
{
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "display the computed nonparametric density estimates for different values of `h`. What is the effect of `h` on density estimates?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def display_hypercube_kernel_density_1D(data, h, color='b'):\n",
    "    xs = np.linspace(min(data), max(data), 200)\n",
    "    plt.plot(xs, list(map(lambda x: hypercube_kernel_density(x, data, h, 1), xs)), '-' + color, label='h=' + str(h))\n",
    "    plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display_hypercube_kernel_density_1D(data, 0.05, 'b')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display_hypercube_kernel_density_1D(data, 0.2, 'b')\n",
    "display_hypercube_kernel_density_1D(data, 1, 'g')\n",
    "display_hypercube_kernel_density_1D(data, 2, 'm')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2) Gaussian Kernel function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Write the function `gaussian_kernel_function` that takes pair of points `x` and `x_n`, size `h` and returns Gaussian kernel function value for the pair of points."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gaussian_kernel_function(x, x_n, h):\n",
    "    pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Write the `gaussian_kernel_density` function that takes any point `x`, training data points `data`, size `h` and returns the Gaussian kernel density value for the point `x` (see the last equation in slide 19 of lecture 7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gaussian_kernel_density(x, data, h):\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def display_gaussian_kernel_density_1D(data, h, color='b'):\n",
    "    xs = np.linspace(min(data), max(data), 200)\n",
    "    plt.plot(xs, list(map(lambda x: gaussian_kernel_density(x, data, h), xs)), '-' + color, label='h=' + str(h))\n",
    "    plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display_gaussian_kernel_density_1D(data, 0.01, 'b')\n",
    "display_gaussian_kernel_density_1D(data, 0.05, 'g')\n",
    "display_gaussian_kernel_density_1D(data, 0.2, 'r')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3) K-Nearest Neigbours classification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1) Generate Data"
   ]
  },
{
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using IRIS dataset from the sklearn library. Note that the data is 2-dimensional (two features). Plot the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "iris = datasets.load_iris()\n",
    "iris_x = np.array(iris.data[:, :2])  # we only take the first two features.\n",
    "iris_t = np.array(iris.target)\n",
    "\n",
    "def plot_iris(legend=True, classes=iris_t, target=plt):\n",
    "    scatter = target.scatter(iris_x[:, 0], iris_x[:, 1], c=classes, alpha=0.7, cmap='rainbow', edgecolor='none')\n",
    "    if legend:\n",
    "        legend = target.legend(*scatter.legend_elements(), loc=\"upper left\", title=\"Classes\")\n",
    "        return (scatter, legend)\n",
    "    return (scatter, )\n",
    "\n",
    "plot_iris()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2) Classification function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "write the `k_nearest_classification` function that takes a test data point `x`, training data points `data_x` and their associated classes `data_t`, the neighbours `k` and returns the predicted class for the test point `x`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def k_nearest_classification(x, data_x, data_t, k):\n",
    "    pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.3) Display results"
   ]
  },
{
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Display the classification results for different values of `K`"
   ]
  },

  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_mesh(pred_fn, n_class=3, x_min=4, x_max=8, y_min=2, y_max=4.5, target=plt):\n",
    "    h = 0.1  # step size in the mesh\n",
    "    xx, yy = np.meshgrid(np.arange(x_min, x_max, h), np.arange(y_min, y_max, h))\n",
    "    Z = np.array(list(map(lambda x: pred_fn(np.array(x)), np.c_[xx.ravel(), yy.ravel()])))\n",
    "    Z = Z.reshape(xx.shape)\n",
    "    cs = target.contourf(xx, yy, Z, alpha = 0.1, cmap=plt.cm.get_cmap('rainbow', n_class))\n",
    "    target.axis('tight')\n",
    "    if hasattr(target, 'xlim'):\n",
    "        target.xlim(x_min, x_max)\n",
    "        target.ylim(y_min, y_max)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_iris(False)\n",
    "plot_mesh(lambda x: k_nearest_classification(x, iris_x, iris_t, 1))\n",
    "plt.legend([], loc=\"upper left\", title=\"K=1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_iris(False)\n",
    "plot_mesh(lambda x: k_nearest_classification(x, iris_x, iris_t, 4))\n",
    "plt.legend([], loc=\"upper left\", title=\"K=4\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_iris(False)\n",
    "plot_mesh(lambda x: k_nearest_classification(x, iris_x, iris_t, 8))\n",
    "plt.legend([], loc=\"upper left\", title=\"K=8\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4) K-Means clustering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "write the `k_means_step` function that takes initial means vector `mus_0`, input data points `data_x` and returns the new means `mus` and `classes` after running a single iteration of the K-means algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def k_means_step(mus_0, data_x):\n",
    "    pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "the following function, `distortion_measure`, takes means `mus`, `classes`, data points `data_x` and returns distortion of this classification (see slide 8 of lecture 8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def distortion_measure(mus, classes, data_x):\n",
    "    \n",
    "    result = 0\n",
    "    \n",
    "    for i, c in enumerate(classes):\n",
    "        \n",
    "        x = data_x[i]\n",
    "        mu = mus[c]\n",
    "        distance = np.dot(np.array(mu) - np.array(x), np.array(mu) - np.array(x))\n",
    "        result += distance\n",
    "    \n",
    "    return result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`optimize_k_means` takes initial means `mus_0`, data points `data_x` and callback `on_step`. `on_step` is a function that takes current `mus` and `classes` and should be called each step. The function will be used later to apply multiple steps of the k-means algorithm to converge on the `true` mean and corresponding classes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def optimize_k_means(mus_0, data_x, on_step):\n",
    "    pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.1) Display results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_k_means(mus, classes, target=plt):\n",
    "    plot = plot_iris(classes=classes, target=target)\n",
    "    scatter = target.scatter(mus[:, 0], mus[:, 1], c=[0, 1, 2], cmap='rainbow', marker='X', s=300, edgecolors='black')\n",
    "    return (*plot, scatter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "k = 3\n",
    "mus_0 = iris_x[:k]\n",
    "\n",
    "all_steps = []\n",
    "\n",
    "optimize_k_means(mus_0, iris_x, lambda mus, classes: all_steps.append((mus, classes)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_animation(all_steps, data_x):\n",
    "    \n",
    "    distortions = list(map(\n",
    "        lambda a: distortion_measure(a[0], a[1], data_x),\n",
    "        all_steps\n",
    "    ))\n",
    "    \n",
    "    fig, (ax, ax2) = plt.subplots(1, 2, figsize=(15,5))\n",
    "    \n",
    "    def animate(i):\n",
    "        ax.cla()\n",
    "        ax2.cla()\n",
    "        \n",
    "        plot1 = plot_k_means(all_steps[i][0], all_steps[i][1], target=ax)\n",
    "        ax2.plot(list(range(i)), distortions[:i], '-o')\n",
    "        plt.xlabel('Step')\n",
    "        plt.ylabel('Distortion')\n",
    "        return plot1\n",
    "    \n",
    "    anim = FuncAnimation(\n",
    "        fig, animate,\n",
    "        frames=len(all_steps), interval=500, blit=True\n",
    "    )\n",
    "    return HTML(anim.to_html5_video())\n",
    "\n",
    "create_animation(all_steps, iris_x)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
