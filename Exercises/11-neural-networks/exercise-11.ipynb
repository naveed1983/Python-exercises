{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Week 13 - Neural Networks\n",
    "\n",
    "Learning contents:\n",
    "- Convolutional Neural Network\n",
    "    - Implement the LeNet-5 inspired architecture in PyTorch\n",
    "- Limited labelled data\n",
    "    - Rerun the LeNet-5 network with only 100 examples total\n",
    "- Implement an Autoencoder and train it on the unlabelled data\n",
    "- Demo: Using the Autoencoder in a multi-task learning setup to improve classification performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports you will need\n",
    "import torch\n",
    "from math import ceil\n",
    "from torchvision import datasets, transforms\n",
    "from torch.utils.data import DataLoader\n",
    "import matplotlib.pyplot as plt\n",
    "from torch.optim import SGD\n",
    "from torch import tanh, flatten\n",
    "from torch.utils.data import DataLoader, random_split\n",
    "from torch.nn import Conv2d, AvgPool2d, Linear, Module, ConvTranspose2d, Upsample, Tanh, Sequential\n",
    "from torch.nn.functional import log_softmax, nll_loss, mse_loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dataset\n",
    "We'll be using the MNIST dataset again this time:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "transform = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.1307,), (0.3081,))\n",
    "])\n",
    "\n",
    "train_data = datasets.MNIST('../data', train=True, download=True, transform=transform)\n",
    "test_data = datasets.MNIST('../data', train=False, download=True, transform=transform)\n",
    "\n",
    "train_loader = DataLoader(\n",
    "    train_data,\n",
    "    batch_size = 64,\n",
    "    shuffle=True,\n",
    ")\n",
    "\n",
    "test_loader = DataLoader(\n",
    "    test_data,\n",
    "    batch_size = 64,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([64, 1, 28, 28]), torch.Size([64]))"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data, targets = next(iter(train_loader))\n",
    "data.shape, targets.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 6 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "fig = plt.figure()\n",
    "for i in range(6):\n",
    "    plt.subplot(2,3,i+1)\n",
    "    plt.tight_layout()\n",
    "    plt.imshow(data[i][0], cmap='gray', interpolation='none')\n",
    "    plt.title(\"Ground Truth: {}\".format(targets[i]))\n",
    "    plt.xticks([])\n",
    "    plt.yticks([])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1) Convolutional Neural Network\n",
    "In this exercise, we will implement a Convolutional Neural Network inspired by the LeNet-5 architecture.\n",
    "\n",
    "![](lenet.png)\n",
    "Source: http://vision.stanford.edu/cs598_spring07/papers/Lecun98.pdf\n",
    "\n",
    "Unlike the original architecture, for the subsampling we will use Averarge Pooling, and our output layer will be another Linear layer."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.1) Implement the LeNet-5 inspired architecture in PyTorch\n",
    "The architecture should consist of the following layers:\n",
    "- Convolutional layer with  6 neurons, a 5x5 kernel, and stride 1, followed by a tanh function\n",
    "- Average pooling layer with a 2x2 window size\n",
    "- Convolutional layer with 16 neurons, a 5x5 kernel, and stride 1, followed by a tanh function\n",
    "- Average pooling layer with a 2x2 window size\n",
    "- Fully connected layer with 120 neurons\n",
    "- Fully connected layer with 84 neurons"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LeNet5(Module):\n",
    "    def __init__(self):\n",
    "        super(LeNet5, self).__init__()\n",
    "        # Initialise the layers here. Use Conv2d, AvgPool2d, Linear, and Tanh\n",
    "        # ...\n",
    "        \n",
    "\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Propagate the data through the layers\n",
    "        # x = self.mylayer(x)\n",
    "        # ...\n",
    "        return log_softmax(x, dim=1)\n",
    "\n",
    "    \n",
    "def train(\n",
    "    model:Module, \n",
    "    train_loader:DataLoader, \n",
    "    optimizer: SGD, \n",
    "    epoch:int, \n",
    "    log_interval = 50\n",
    "):\n",
    "    model.train()\n",
    "    for batch_idx, (data, target) in enumerate(train_loader):\n",
    "        # Reset the gradients\n",
    "        optimizer.zero_grad() \n",
    "        \n",
    "        # Feed the data through the model\n",
    "        output = model(data) \n",
    "        \n",
    "        # Compute the negative log-likelihood loss\n",
    "        loss = nll_loss(output, target) \n",
    "        \n",
    "        # Backward propagate the gradients\n",
    "        loss.backward() \n",
    "        \n",
    "        # Perform an update step using the optimizer\n",
    "        optimizer.step() \n",
    "        \n",
    "        # Log\n",
    "        if batch_idx % log_interval == 0:\n",
    "            print('Train Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}'.format(\n",
    "                epoch, batch_idx * len(data), len(train_loader.dataset),\n",
    "                100. * batch_idx / len(train_loader), loss.item()))\n",
    "\n",
    "\n",
    "def test(model:Module, test_loader:DataLoader):\n",
    "    # Set model to evaluation mode\n",
    "    model.eval()\n",
    "    \n",
    "    test_loss = 0\n",
    "    correct = 0\n",
    "    \n",
    "    # Don't accumulate gradients\n",
    "    with torch.no_grad():\n",
    "        for data, target in test_loader:\n",
    "            # Feed the data through the model\n",
    "            output = model(data)\n",
    "            \n",
    "            # Sum up batch loss\n",
    "            test_loss += nll_loss(output, target, reduction='sum').item()  \n",
    "            \n",
    "            # The prediction is the index of the max log-probability\n",
    "            pred = output.argmax(dim=1, keepdim=True)  \n",
    "            \n",
    "            correct += pred.eq(target.view_as(pred)).sum().item()\n",
    "\n",
    "    test_loss /= len(test_loader.dataset)\n",
    "\n",
    "    print('\\nTest set: Average loss: {:.4f}, Accuracy: {}/{} ({:.3f}%)\\n'.format(\n",
    "        test_loss, correct, len(test_loader.dataset),\n",
    "        100. * correct / len(test_loader.dataset)\n",
    "    ))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Train the network for 10 epochs using SGD with a `learning_rate = 0.01` and `momentum = 0.9`.\n",
    "You should be able to get a 99% accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = LeNet5()\n",
    "optimizer = SGD(model.parameters(), lr=0.01, momentum=0.9)\n",
    "\n",
    "epochs = 10\n",
    "for epoch in range(1, epochs + 1):\n",
    "    train(model, train_loader, optimizer, epoch, log_interval=50)\n",
    "    test(model, test_loader)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2) Limited labelled data\n",
    "In the previous case, we had plenty of training data (60000 example) to get good performance.\n",
    "What if we had much less data (100 examples)?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate new datasets\n",
    "mini_train_size = 100\n",
    "\n",
    "mini_train_data, rest_train_data = random_split(  \n",
    "    train_data, [mini_train_size, len(train_data) - mini_train_size]\n",
    ")\n",
    "\n",
    "# Our small labelled dataset\n",
    "mini_train_loader = DataLoader(\n",
    "    mini_train_data,\n",
    "    batch_size = 64,\n",
    "    shuffle=True,\n",
    ")\n",
    "\n",
    "# Our larger, unlabbeled dataset\n",
    "# We will act as if we didn't have labels for this data\n",
    "rest_train_loader = DataLoader(\n",
    "    rest_train_data,\n",
    "    batch_size = 64,\n",
    "    shuffle=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.1) Rerun the LeNet-5 network with only 100 examples total\n",
    "Run it for 200 epochs using the `mini_train_loader`.\n",
    "What is the test accuracy?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 1 [0/100 (0%)]\tLoss: 2.283840\n",
      "Train Epoch: 2 [0/100 (0%)]\tLoss: 2.289171\n",
      "Train Epoch: 3 [0/100 (0%)]\tLoss: 2.265617\n",
      "Train Epoch: 4 [0/100 (0%)]\tLoss: 2.257952\n",
      "Train Epoch: 5 [0/100 (0%)]\tLoss: 2.246766\n",
      "Train Epoch: 6 [0/100 (0%)]\tLoss: 2.226400\n",
      "Train Epoch: 7 [0/100 (0%)]\tLoss: 2.222587\n",
      "Train Epoch: 8 [0/100 (0%)]\tLoss: 2.188046\n",
      "Train Epoch: 9 [0/100 (0%)]\tLoss: 2.192306\n",
      "Train Epoch: 10 [0/100 (0%)]\tLoss: 2.184013\n",
      "\n",
      "Test set: Average loss: 2.2471, Accuracy: 2163/10000 (21.630%)\n",
      "\n",
      "Train Epoch: 11 [0/100 (0%)]\tLoss: 2.146529\n",
      "Train Epoch: 12 [0/100 (0%)]\tLoss: 2.119649\n",
      "Train Epoch: 13 [0/100 (0%)]\tLoss: 2.135626\n",
      "Train Epoch: 14 [0/100 (0%)]\tLoss: 2.101483\n",
      "Train Epoch: 15 [0/100 (0%)]\tLoss: 2.091165\n",
      "Train Epoch: 16 [0/100 (0%)]\tLoss: 2.070218\n",
      "Train Epoch: 17 [0/100 (0%)]\tLoss: 2.000441\n",
      "Train Epoch: 18 [0/100 (0%)]\tLoss: 1.988960\n",
      "Train Epoch: 19 [0/100 (0%)]\tLoss: 1.966821\n",
      "Train Epoch: 20 [0/100 (0%)]\tLoss: 1.978968\n",
      "\n",
      "Test set: Average loss: 2.0732, Accuracy: 3354/10000 (33.540%)\n",
      "\n",
      "Train Epoch: 21 [0/100 (0%)]\tLoss: 1.955305\n",
      "Train Epoch: 22 [0/100 (0%)]\tLoss: 1.953573\n",
      "Train Epoch: 23 [0/100 (0%)]\tLoss: 1.842418\n",
      "Train Epoch: 24 [0/100 (0%)]\tLoss: 1.872639\n",
      "Train Epoch: 25 [0/100 (0%)]\tLoss: 1.804975\n",
      "Train Epoch: 26 [0/100 (0%)]\tLoss: 1.761862\n",
      "Train Epoch: 27 [0/100 (0%)]\tLoss: 1.740003\n",
      "Train Epoch: 28 [0/100 (0%)]\tLoss: 1.706900\n",
      "Train Epoch: 29 [0/100 (0%)]\tLoss: 1.682997\n",
      "Train Epoch: 30 [0/100 (0%)]\tLoss: 1.688409\n",
      "\n",
      "Test set: Average loss: 1.7882, Accuracy: 4832/10000 (48.320%)\n",
      "\n",
      "Train Epoch: 31 [0/100 (0%)]\tLoss: 1.624950\n",
      "Train Epoch: 32 [0/100 (0%)]\tLoss: 1.603279\n",
      "Train Epoch: 33 [0/100 (0%)]\tLoss: 1.583271\n",
      "Train Epoch: 34 [0/100 (0%)]\tLoss: 1.587475\n",
      "Train Epoch: 35 [0/100 (0%)]\tLoss: 1.540611\n",
      "Train Epoch: 36 [0/100 (0%)]\tLoss: 1.558447\n",
      "Train Epoch: 37 [0/100 (0%)]\tLoss: 1.485837\n",
      "Train Epoch: 38 [0/100 (0%)]\tLoss: 1.459781\n",
      "Train Epoch: 39 [0/100 (0%)]\tLoss: 1.475217\n",
      "Train Epoch: 40 [0/100 (0%)]\tLoss: 1.468594\n",
      "\n",
      "Test set: Average loss: 1.6209, Accuracy: 5514/10000 (55.140%)\n",
      "\n",
      "Train Epoch: 41 [0/100 (0%)]\tLoss: 1.416816\n",
      "Train Epoch: 42 [0/100 (0%)]\tLoss: 1.425992\n",
      "Train Epoch: 43 [0/100 (0%)]\tLoss: 1.384188\n",
      "Train Epoch: 44 [0/100 (0%)]\tLoss: 1.380180\n",
      "Train Epoch: 45 [0/100 (0%)]\tLoss: 1.385396\n",
      "Train Epoch: 46 [0/100 (0%)]\tLoss: 1.354397\n",
      "Train Epoch: 47 [0/100 (0%)]\tLoss: 1.338146\n",
      "Train Epoch: 48 [0/100 (0%)]\tLoss: 1.307256\n",
      "Train Epoch: 49 [0/100 (0%)]\tLoss: 1.318801\n",
      "Train Epoch: 50 [0/100 (0%)]\tLoss: 1.305901\n",
      "\n",
      "Test set: Average loss: 1.5143, Accuracy: 5985/10000 (59.850%)\n",
      "\n",
      "Train Epoch: 51 [0/100 (0%)]\tLoss: 1.272952\n",
      "Train Epoch: 52 [0/100 (0%)]\tLoss: 1.272852\n",
      "Train Epoch: 53 [0/100 (0%)]\tLoss: 1.259100\n",
      "Train Epoch: 54 [0/100 (0%)]\tLoss: 1.235495\n",
      "Train Epoch: 55 [0/100 (0%)]\tLoss: 1.251170\n",
      "Train Epoch: 56 [0/100 (0%)]\tLoss: 1.248161\n",
      "Train Epoch: 57 [0/100 (0%)]\tLoss: 1.254049\n",
      "Train Epoch: 58 [0/100 (0%)]\tLoss: 1.220269\n",
      "Train Epoch: 59 [0/100 (0%)]\tLoss: 1.241511\n",
      "Train Epoch: 60 [0/100 (0%)]\tLoss: 1.214725\n",
      "\n",
      "Test set: Average loss: 1.4582, Accuracy: 6033/10000 (60.330%)\n",
      "\n",
      "Train Epoch: 61 [0/100 (0%)]\tLoss: 1.218211\n",
      "Train Epoch: 62 [0/100 (0%)]\tLoss: 1.225103\n",
      "Train Epoch: 63 [0/100 (0%)]\tLoss: 1.205738\n",
      "Train Epoch: 64 [0/100 (0%)]\tLoss: 1.183838\n",
      "Train Epoch: 65 [0/100 (0%)]\tLoss: 1.177820\n",
      "Train Epoch: 66 [0/100 (0%)]\tLoss: 1.190223\n",
      "Train Epoch: 67 [0/100 (0%)]\tLoss: 1.142229\n",
      "Train Epoch: 68 [0/100 (0%)]\tLoss: 1.181830\n",
      "Train Epoch: 69 [0/100 (0%)]\tLoss: 1.166019\n",
      "Train Epoch: 70 [0/100 (0%)]\tLoss: 1.124071\n",
      "\n",
      "Test set: Average loss: 1.4199, Accuracy: 6137/10000 (61.370%)\n",
      "\n",
      "Train Epoch: 71 [0/100 (0%)]\tLoss: 1.146330\n",
      "Train Epoch: 72 [0/100 (0%)]\tLoss: 1.151952\n",
      "Train Epoch: 73 [0/100 (0%)]\tLoss: 1.131531\n",
      "Train Epoch: 74 [0/100 (0%)]\tLoss: 1.151115\n",
      "Train Epoch: 75 [0/100 (0%)]\tLoss: 1.157517\n",
      "Train Epoch: 76 [0/100 (0%)]\tLoss: 1.112594\n",
      "Train Epoch: 77 [0/100 (0%)]\tLoss: 1.136842\n",
      "Train Epoch: 78 [0/100 (0%)]\tLoss: 1.149369\n",
      "Train Epoch: 79 [0/100 (0%)]\tLoss: 1.122514\n",
      "Train Epoch: 80 [0/100 (0%)]\tLoss: 1.128344\n",
      "\n",
      "Test set: Average loss: 1.3978, Accuracy: 6147/10000 (61.470%)\n",
      "\n",
      "Train Epoch: 81 [0/100 (0%)]\tLoss: 1.116331\n",
      "Train Epoch: 82 [0/100 (0%)]\tLoss: 1.118600\n",
      "Train Epoch: 83 [0/100 (0%)]\tLoss: 1.110805\n",
      "Train Epoch: 84 [0/100 (0%)]\tLoss: 1.100303\n",
      "Train Epoch: 85 [0/100 (0%)]\tLoss: 1.099210\n",
      "Train Epoch: 86 [0/100 (0%)]\tLoss: 1.096173\n",
      "Train Epoch: 87 [0/100 (0%)]\tLoss: 1.088248\n",
      "Train Epoch: 88 [0/100 (0%)]\tLoss: 1.077654\n",
      "Train Epoch: 89 [0/100 (0%)]\tLoss: 1.097283\n",
      "Train Epoch: 90 [0/100 (0%)]\tLoss: 1.069239\n",
      "\n",
      "Test set: Average loss: 1.3736, Accuracy: 6241/10000 (62.410%)\n",
      "\n",
      "Train Epoch: 91 [0/100 (0%)]\tLoss: 1.087191\n",
      "Train Epoch: 92 [0/100 (0%)]\tLoss: 1.092869\n",
      "Train Epoch: 93 [0/100 (0%)]\tLoss: 1.079476\n",
      "Train Epoch: 94 [0/100 (0%)]\tLoss: 1.062963\n",
      "Train Epoch: 95 [0/100 (0%)]\tLoss: 1.080730\n",
      "Train Epoch: 96 [0/100 (0%)]\tLoss: 1.080183\n",
      "Train Epoch: 97 [0/100 (0%)]\tLoss: 1.062920\n",
      "Train Epoch: 98 [0/100 (0%)]\tLoss: 1.032282\n",
      "Train Epoch: 99 [0/100 (0%)]\tLoss: 1.027508\n",
      "Train Epoch: 100 [0/100 (0%)]\tLoss: 1.050167\n",
      "\n",
      "Test set: Average loss: 1.3565, Accuracy: 6290/10000 (62.900%)\n",
      "\n",
      "Train Epoch: 101 [0/100 (0%)]\tLoss: 1.039502\n",
      "Train Epoch: 102 [0/100 (0%)]\tLoss: 1.040623\n",
      "Train Epoch: 103 [0/100 (0%)]\tLoss: 1.037048\n",
      "Train Epoch: 104 [0/100 (0%)]\tLoss: 1.039375\n",
      "Train Epoch: 105 [0/100 (0%)]\tLoss: 1.045967\n",
      "Train Epoch: 106 [0/100 (0%)]\tLoss: 1.041520\n",
      "Train Epoch: 107 [0/100 (0%)]\tLoss: 1.052941\n",
      "Train Epoch: 108 [0/100 (0%)]\tLoss: 1.024090\n",
      "Train Epoch: 109 [0/100 (0%)]\tLoss: 1.031536\n",
      "Train Epoch: 110 [0/100 (0%)]\tLoss: 1.029661\n",
      "\n",
      "Test set: Average loss: 1.3393, Accuracy: 6350/10000 (63.500%)\n",
      "\n",
      "Train Epoch: 111 [0/100 (0%)]\tLoss: 0.989413\n",
      "Train Epoch: 112 [0/100 (0%)]\tLoss: 1.036775\n",
      "Train Epoch: 113 [0/100 (0%)]\tLoss: 1.013190\n",
      "Train Epoch: 114 [0/100 (0%)]\tLoss: 1.009953\n",
      "Train Epoch: 115 [0/100 (0%)]\tLoss: 1.010225\n",
      "Train Epoch: 116 [0/100 (0%)]\tLoss: 1.018463\n",
      "Train Epoch: 117 [0/100 (0%)]\tLoss: 1.007927\n",
      "Train Epoch: 118 [0/100 (0%)]\tLoss: 1.002908\n",
      "Train Epoch: 119 [0/100 (0%)]\tLoss: 1.013098\n",
      "Train Epoch: 120 [0/100 (0%)]\tLoss: 0.995098\n",
      "\n",
      "Test set: Average loss: 1.3207, Accuracy: 6454/10000 (64.540%)\n",
      "\n",
      "Train Epoch: 121 [0/100 (0%)]\tLoss: 0.995725\n",
      "Train Epoch: 122 [0/100 (0%)]\tLoss: 0.990947\n",
      "Train Epoch: 123 [0/100 (0%)]\tLoss: 0.987570\n",
      "Train Epoch: 124 [0/100 (0%)]\tLoss: 0.995200\n",
      "Train Epoch: 125 [0/100 (0%)]\tLoss: 0.983761\n",
      "Train Epoch: 126 [0/100 (0%)]\tLoss: 0.999954\n",
      "Train Epoch: 127 [0/100 (0%)]\tLoss: 0.977264\n",
      "Train Epoch: 128 [0/100 (0%)]\tLoss: 1.002895\n",
      "Train Epoch: 129 [0/100 (0%)]\tLoss: 0.988096\n",
      "Train Epoch: 130 [0/100 (0%)]\tLoss: 0.990299\n",
      "\n",
      "Test set: Average loss: 1.3117, Accuracy: 6469/10000 (64.690%)\n",
      "\n",
      "Train Epoch: 131 [0/100 (0%)]\tLoss: 0.969368\n",
      "Train Epoch: 132 [0/100 (0%)]\tLoss: 0.951817\n",
      "Train Epoch: 133 [0/100 (0%)]\tLoss: 0.992096\n",
      "Train Epoch: 134 [0/100 (0%)]\tLoss: 0.980204\n",
      "Train Epoch: 135 [0/100 (0%)]\tLoss: 0.961356\n",
      "Train Epoch: 136 [0/100 (0%)]\tLoss: 0.979868\n",
      "Train Epoch: 137 [0/100 (0%)]\tLoss: 0.976504\n",
      "Train Epoch: 138 [0/100 (0%)]\tLoss: 0.959938\n",
      "Train Epoch: 139 [0/100 (0%)]\tLoss: 0.954893\n",
      "Train Epoch: 140 [0/100 (0%)]\tLoss: 0.958700\n",
      "\n",
      "Test set: Average loss: 1.2987, Accuracy: 6519/10000 (65.190%)\n",
      "\n",
      "Train Epoch: 141 [0/100 (0%)]\tLoss: 0.954308\n",
      "Train Epoch: 142 [0/100 (0%)]\tLoss: 0.967023\n",
      "Train Epoch: 143 [0/100 (0%)]\tLoss: 0.972679\n",
      "Train Epoch: 144 [0/100 (0%)]\tLoss: 0.957446\n",
      "Train Epoch: 145 [0/100 (0%)]\tLoss: 0.935737\n",
      "Train Epoch: 146 [0/100 (0%)]\tLoss: 0.939302\n",
      "Train Epoch: 147 [0/100 (0%)]\tLoss: 0.962787\n",
      "Train Epoch: 148 [0/100 (0%)]\tLoss: 0.962362\n",
      "Train Epoch: 149 [0/100 (0%)]\tLoss: 0.956751\n",
      "Train Epoch: 150 [0/100 (0%)]\tLoss: 0.930829\n",
      "\n",
      "Test set: Average loss: 1.2883, Accuracy: 6602/10000 (66.020%)\n",
      "\n",
      "Train Epoch: 151 [0/100 (0%)]\tLoss: 0.940817\n",
      "Train Epoch: 152 [0/100 (0%)]\tLoss: 0.949006\n",
      "Train Epoch: 153 [0/100 (0%)]\tLoss: 0.950214\n",
      "Train Epoch: 154 [0/100 (0%)]\tLoss: 0.938042\n",
      "Train Epoch: 155 [0/100 (0%)]\tLoss: 0.943643\n",
      "Train Epoch: 156 [0/100 (0%)]\tLoss: 0.958725\n",
      "Train Epoch: 157 [0/100 (0%)]\tLoss: 0.941709\n",
      "Train Epoch: 158 [0/100 (0%)]\tLoss: 0.965082\n",
      "Train Epoch: 159 [0/100 (0%)]\tLoss: 0.919055\n",
      "Train Epoch: 160 [0/100 (0%)]\tLoss: 0.947824\n",
      "\n",
      "Test set: Average loss: 1.2827, Accuracy: 6633/10000 (66.330%)\n",
      "\n",
      "Train Epoch: 161 [0/100 (0%)]\tLoss: 0.932245\n",
      "Train Epoch: 162 [0/100 (0%)]\tLoss: 0.919572\n",
      "Train Epoch: 163 [0/100 (0%)]\tLoss: 0.934806\n",
      "Train Epoch: 164 [0/100 (0%)]\tLoss: 0.934080\n",
      "Train Epoch: 165 [0/100 (0%)]\tLoss: 0.924725\n",
      "Train Epoch: 166 [0/100 (0%)]\tLoss: 0.927253\n",
      "Train Epoch: 167 [0/100 (0%)]\tLoss: 0.914157\n",
      "Train Epoch: 168 [0/100 (0%)]\tLoss: 0.931614\n",
      "Train Epoch: 169 [0/100 (0%)]\tLoss: 0.934387\n",
      "Train Epoch: 170 [0/100 (0%)]\tLoss: 0.938260\n",
      "\n",
      "Test set: Average loss: 1.2741, Accuracy: 6683/10000 (66.830%)\n",
      "\n",
      "Train Epoch: 171 [0/100 (0%)]\tLoss: 0.942368\n",
      "Train Epoch: 172 [0/100 (0%)]\tLoss: 0.919907\n",
      "Train Epoch: 173 [0/100 (0%)]\tLoss: 0.929616\n",
      "Train Epoch: 174 [0/100 (0%)]\tLoss: 0.932267\n",
      "Train Epoch: 175 [0/100 (0%)]\tLoss: 0.921473\n",
      "Train Epoch: 176 [0/100 (0%)]\tLoss: 0.914476\n",
      "Train Epoch: 177 [0/100 (0%)]\tLoss: 0.924020\n",
      "Train Epoch: 178 [0/100 (0%)]\tLoss: 0.926950\n",
      "Train Epoch: 179 [0/100 (0%)]\tLoss: 0.910011\n",
      "Train Epoch: 180 [0/100 (0%)]\tLoss: 0.924217\n",
      "\n",
      "Test set: Average loss: 1.2727, Accuracy: 6657/10000 (66.570%)\n",
      "\n",
      "Train Epoch: 181 [0/100 (0%)]\tLoss: 0.907868\n",
      "Train Epoch: 182 [0/100 (0%)]\tLoss: 0.912834\n",
      "Train Epoch: 183 [0/100 (0%)]\tLoss: 0.917973\n",
      "Train Epoch: 184 [0/100 (0%)]\tLoss: 0.916427\n",
      "Train Epoch: 185 [0/100 (0%)]\tLoss: 0.930693\n",
      "Train Epoch: 186 [0/100 (0%)]\tLoss: 0.908350\n",
      "Train Epoch: 187 [0/100 (0%)]\tLoss: 0.879743\n",
      "Train Epoch: 188 [0/100 (0%)]\tLoss: 0.916999\n",
      "Train Epoch: 189 [0/100 (0%)]\tLoss: 0.893066\n",
      "Train Epoch: 190 [0/100 (0%)]\tLoss: 0.887590\n",
      "\n",
      "Test set: Average loss: 1.2669, Accuracy: 6693/10000 (66.930%)\n",
      "\n",
      "Train Epoch: 191 [0/100 (0%)]\tLoss: 0.895083\n",
      "Train Epoch: 192 [0/100 (0%)]\tLoss: 0.912505\n",
      "Train Epoch: 193 [0/100 (0%)]\tLoss: 0.915177\n",
      "Train Epoch: 194 [0/100 (0%)]\tLoss: 0.896392\n",
      "Train Epoch: 195 [0/100 (0%)]\tLoss: 0.909797\n",
      "Train Epoch: 196 [0/100 (0%)]\tLoss: 0.902858\n",
      "Train Epoch: 197 [0/100 (0%)]\tLoss: 0.906730\n",
      "Train Epoch: 198 [0/100 (0%)]\tLoss: 0.906077\n",
      "Train Epoch: 199 [0/100 (0%)]\tLoss: 0.897573\n",
      "Train Epoch: 200 [0/100 (0%)]\tLoss: 0.908678\n",
      "\n",
      "Test set: Average loss: 1.2633, Accuracy: 6719/10000 (67.190%)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "model = LeNet5()\n",
    "optimizer = SGD(model.parameters(), lr=0.01, momentum=0.9)\n",
    "\n",
    "epochs = 200\n",
    "for epoch in range(1, epochs + 1):\n",
    "    train(model, mini_train_loader, optimizer, epoch)\n",
    "    if epoch % 10 == 0:\n",
    "        test(model, test_loader)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We end with ≈67% accuracy. NB: due to the small size of training data, the variance of the results may be high."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3) Implement an Autoencoder and train it on the unlabelled data\n",
    "Though we have very little labelled training data, we may have plenty of unlabelled data available (`rest_data_loader`). \n",
    "This data can be utilised in many ways; one such way is by using an Autoencoder.\n",
    "\n",
    "Implement an autoencoder with the following layers:\n",
    "- Encoder:\n",
    "    - Convolutional layer with  6 neurons, a 5x5 kernel, and stride 1, followed by a tanh function\n",
    "    - Average pooling layer with a 2x2 window size\n",
    "    - Convolutional layer with 16 neurons, a 5x5 kernel, and stride 1, followed by a tanh function\n",
    "    - Average pooling layer with a 2x2 window size\n",
    "    - Fully connected layer with 120 neurons\n",
    "- Decoder:\n",
    "    - Fully connected layer with 256 neurons\n",
    "    - Upsample layer with scale-factor 2\n",
    "    - Transpose convolution with 16 neurons, a 5x5 kernel, and stride 1, followed by a tanh function\n",
    "    - Upsample layer with scale-factor 2\n",
    "    - Transpose convolution with 6 neurons, a 5x5 kernel, and stride 1, followed by a tanh function\n",
    "    \n",
    "Use the given training code to train check that the implementation works and loss is reduced."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AutoEncoder(Module):\n",
    "    def __init__(self):\n",
    "        super(AutoEncoder, self).__init__()\n",
    "        self.encoder_conv = Sequential(   \n",
    "            # Initialise the convolutional and pooling layers here\n",
    "        )\n",
    "        self.encoder_fc = Linear(???)\n",
    "        self.decoder_fc = Linear(???)\n",
    "        self.decoder_conv = Sequential(     \n",
    "            # Initialise the transpose convolutions and upsampling layers here\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Propagate the data through the layers.\n",
    "        # Hint: beware of the input and output shapes for each layer.\n",
    "        # You will need to do call `reshape` along the way\n",
    "        # ...\n",
    "        \n",
    "        return x\n",
    "\n",
    "def train_autoencoder(\n",
    "    model:Module, \n",
    "    train_loader:DataLoader, \n",
    "    optimizer: SGD, \n",
    "    epoch:int, \n",
    "    log_interval = 50\n",
    "):\n",
    "    model.train()\n",
    "    for batch_idx, (data, _) in enumerate(train_loader):\n",
    "        # Reset the gradients\n",
    "        optimizer.zero_grad() \n",
    "        \n",
    "        # Feed the data through the model\n",
    "        output = model(data) \n",
    "        \n",
    "        # Compute the negative log-likelihood loss\n",
    "        loss = mse_loss(output, data) \n",
    "        \n",
    "        # Backward propagate the gradients\n",
    "        loss.backward() \n",
    "        \n",
    "        # Perform an update step using the optimizer\n",
    "        optimizer.step() \n",
    "        \n",
    "        # Log\n",
    "        if batch_idx % log_interval == 0:\n",
    "            print('AE Train Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}'.format(\n",
    "                epoch, batch_idx * len(data), len(train_loader.dataset),\n",
    "                100. * batch_idx / len(train_loader), loss.item()))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check that the auto-encoder works by training it and checking that the loss is reduced\n",
    "ae_model = AutoEncoder()\n",
    "optimizer = SGD(ae_model.parameters(), lr=0.01, momentum=0.9)\n",
    "\n",
    "batch_size = 64\n",
    "epochs = 1\n",
    "for epoch in range(1, epochs + 1):\n",
    "    train_autoencoder(ae_model, rest_train_loader, optimizer, epoch, log_interval=50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Demo: Using the Autoencoder in a multi-task learning setup to improve classification performance.\n",
    "Multitask learning is the learning of multiple tasks in parallel.\n",
    "What we do here is to integrate the encoding layers of the autoencoder as layers in the LeNet5. We can then train the autoencoder and classification network in parallel in order to improve performance on the classification task.\n",
    "\n",
    "The code below assumes that you have a working implementation of the Autoencoder."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AutoEncoderLeNet5(Module):\n",
    "    def __init__(self, autoencoder):\n",
    "        super(AutoEncoderLeNet5, self).__init__()\n",
    "        self.autoencoder = autoencoder\n",
    "        self.fc2 = Linear(in_features=120, out_features=84)\n",
    "        self.fc3 = Linear(in_features=84, out_features=10)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.autoencoder.encoder_conv(x)\n",
    "        x = flatten(x, 1)\n",
    "        x = self.autoencoder.encoder_fc(x)\n",
    "        x = tanh(self.fc2(x))\n",
    "        x = tanh(self.fc3(x))\n",
    "        return log_softmax(x, dim=1)\n",
    "    \n",
    "def train_hybrid(\n",
    "    model:Module, \n",
    "    ae_model:Module,\n",
    "    labelled_loader:DataLoader,\n",
    "    unlabelled_loader:DataLoader,\n",
    "    optimizer: SGD, \n",
    "    labelled_steps:int,\n",
    "    unlabelled_steps:int,\n",
    "    epoch:int, \n",
    "    log_interval = 50\n",
    "):\n",
    "    ae_model.train()\n",
    "    model.train()\n",
    "    labelled_iter = iter(labelled_loader)\n",
    "    unlabelled_iter = iter(unlabelled_loader)\n",
    "    for step in range(1, unlabelled_steps+1):\n",
    "        if step % labelled_steps == 0:\n",
    "            labelled_iter = iter(labelled_loader)\n",
    "            \n",
    "        data_l, target = next(labelled_iter)\n",
    "        data_u, _ = next(unlabelled_iter)\n",
    "        \n",
    "        # Reset the gradients\n",
    "        optimizer.zero_grad() \n",
    "        \n",
    "        # Feed the data through the model\n",
    "        output_u = ae_model(data_u)\n",
    "        output_l = model(data_l)\n",
    "        \n",
    "        # Compute the negative log-likelihood loss\n",
    "        loss_l = nll_loss(output_l, target) \n",
    "        loss_u = mse_loss(output_u, data_u) \n",
    "        \n",
    "        # Backward propagate the gradients\n",
    "        loss_u.backward() \n",
    "        loss_l.backward() \n",
    "        \n",
    "        # Perform an update step using the optimizer\n",
    "        optimizer.step() \n",
    "        \n",
    "        # Log\n",
    "        if step % log_interval == 0:\n",
    "            print('Train Epoch: {} [{}/{} ({:.0f}%)]\\tULoss: {:.6f} \\t LLoss: {:.6f}'.format(\n",
    "                epoch, step * unlabelled_steps, len(unlabelled_loader.dataset),\n",
    "                100. * step / unlabelled_steps, loss_u.item(), loss_l.item()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ae_model = AutoEncoder()\n",
    "hybrid_model = AutoEncoderLeNet5(ae_model)\n",
    "optimizer = SGD(ae_model.parameters(), lr=0.01, momentum=0.9)\n",
    "\n",
    "batch_size = 64\n",
    "labelled_steps = ceil(len(mini_train_data) / batch_size)\n",
    "unlabelled_steps = ceil(len(rest_train_data) / batch_size)\n",
    "epochs = 5\n",
    "for epoch in range(1, epochs + 1):\n",
    "    train_hybrid(hybrid_model, ae_model, mini_train_loader, rest_train_loader, optimizer, labelled_steps, unlabelled_steps, epoch, log_interval=50)\n",
    "    test(hybrid_model, test_loader)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You should expect an classification accuracy boost of 10-15% compared to using only the `mini_train_loader`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
